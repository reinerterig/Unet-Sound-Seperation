{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, LeakyReLU, UpSampling1D, Concatenate, Subtract\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Cropping1D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.nn import sigmoid\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/rei/Documents/Machine_Learning/MODELS/Unet/Shaking Through Sound Seperation/Shaking_Through_Model_01.keras\"\n",
    "trainDir = '/Users/rei/Documents/Machine_Learning/Data/Audio/Shaking_Through/Dataset/Train'\n",
    "testDir = '/Users/rei/Documents/Machine_Learning/Data/Audio/Shaking_Through/Dataset/Test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(tensor, target_shape, match_feature_dim=True):\n",
    "    shape = tf.shape(tensor)\n",
    "    diff = shape - target_shape\n",
    "    assert diff[1] >= 0 # Only positive difference allowed\n",
    "    if diff[1] == 0:\n",
    "        return tensor\n",
    "    crop_start = diff // 2\n",
    "    crop_end = diff - crop_start\n",
    "    return tensor[:, crop_start[1]:-crop_end[1], :]\n",
    "\n",
    "def AudioClip(x, training):\n",
    "    if training:\n",
    "        return x\n",
    "    else:\n",
    "        return tf.maximum(tf.minimum(x, 1.0), -1.0)\n",
    "\n",
    "def difference_output(input_mix, featuremap, source_names, num_channels, filter_width, padding, activation, training):\n",
    "    outputs = dict()\n",
    "    sum_source = 0\n",
    "    for name in source_names[:-1]:\n",
    "        out = tf.keras.layers.Conv1D(num_channels, filter_width, activation=activation, padding=padding)(featuremap)\n",
    "        outputs[name] = out\n",
    "        sum_source += out\n",
    "\n",
    "    last_source = crop(input_mix, sum_source.shape) - sum_source\n",
    "    last_source = AudioClip(last_source, training)\n",
    "    outputs[source_names[-1]] = last_source\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learned_interpolation_layer(input, padding, level):\n",
    "    features = input.shape[2]\n",
    "    weights = tf.Variable(tf.initializers.GlorotUniform()(shape=[features]), dtype=tf.float32, name=\"interp_\" + str(level))\n",
    "    weights_scaled = tf.nn.sigmoid(weights)\n",
    "    counter_weights = 1.0 - weights_scaled\n",
    "\n",
    "    conv_weights = tf.linalg.diag(weights_scaled)\n",
    "    conv_weights = tf.expand_dims(conv_weights, axis=0)\n",
    "    intermediate_vals = tf.linalg.matmul(input, conv_weights)\n",
    "    \n",
    "    counter_conv_weights = tf.linalg.diag(counter_weights)\n",
    "    counter_conv_weights = tf.expand_dims(counter_conv_weights, axis=0)\n",
    "    counter_intermediate_vals = tf.linalg.matmul(input, counter_conv_weights)\n",
    "\n",
    "    output = tf.concat([intermediate_vals, counter_intermediate_vals], axis=1)\n",
    "    \n",
    "    if padding == \"valid\":\n",
    "        output = output[:, :-1, :]\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(input, num_layers, num_initial_filters, filter_size, input_filter_size, padding):\n",
    "    enc_outputs = []\n",
    "    current_layer = input\n",
    "    current_layer = tf.keras.layers.Conv1D(num_initial_filters, input_filter_size, strides=1, activation=LeakyReLU(), padding=padding)(current_layer)\n",
    "    enc_outputs.append(current_layer)\n",
    "\n",
    "    for i in range(num_layers - 1):\n",
    "        current_layer = tf.keras.layers.Conv1D(num_initial_filters + (num_initial_filters * i), filter_size, strides=1, activation=LeakyReLU(), padding=padding)(current_layer)\n",
    "        current_layer = current_layer[:, ::2, :]  # Decimate by factor of 2\n",
    "        enc_outputs.append(current_layer)\n",
    "\n",
    "    return enc_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(enc_outputs, num_layers, num_initial_filters, filter_size, merge_filter_size, padding, upsampling):\n",
    "    current_layer = enc_outputs[-1]\n",
    "\n",
    "    for i in range(num_layers - 1, 0, -1):\n",
    "        if upsampling == 'linear':\n",
    "            current_layer = tf.keras.layers.UpSampling1D(size=2)(current_layer)\n",
    "        elif upsampling == 'learned':\n",
    "            current_layer = learned_interpolation_layer(current_layer, padding=padding, level=i)\n",
    "\n",
    "        current_layer = tf.concat([current_layer, enc_outputs[i - 1]], axis=2)\n",
    "        current_layer = tf.keras.layers.Conv1D(num_initial_filters * (num_layers - i), merge_filter_size, strides=1, activation=LeakyReLU(), padding=padding)(current_layer)\n",
    "\n",
    "    return current_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layer(current_layer, output_type, source_names, num_channels, output_filter_size, padding, activation, training):\n",
    "    if output_type == \"direct\":\n",
    "        return independent_outputs(current_layer, source_names, num_channels, output_filter_size, padding, activation)\n",
    "    elif output_type == \"difference\":\n",
    "        cropped_input = crop(input, current_layer.get_shape().as_list(), match_feature_dim=False)\n",
    "        return difference_output(cropped_input, current_layer, source_names, num_channels, output_filter_size, padding, activation, training)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown output type\")\n",
    "\n",
    "def independent_outputs(featuremap, source_names, num_channels, filter_width, padding, activation):\n",
    "    outputs = dict()\n",
    "    for name in source_names:\n",
    "        outputs[name] = tf.keras.layers.Conv1D(num_channels, filter_width, activation=activation, padding=padding)(featuremap)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 16384\n",
    "num_channels = 1\n",
    "num_layers = 12\n",
    "num_initial_filters = 24\n",
    "filter_size = 15\n",
    "merge_filter_size = 5\n",
    "input_filter_size = 15\n",
    "output_filter_size = 1\n",
    "padding = 'same'  \n",
    "upsampling = 'linear'  # or 'learned'\n",
    "output_type = 'direct'  # or 'difference'\n",
    "source_names = [\"accompaniment\", \"vocals\"]\n",
    "activation = 'tanh'\n",
    "training = True\n",
    "\n",
    "def build_model():\n",
    "    # Input\n",
    "    input_mix = Input(shape=(num_frames, num_channels), name=\"input\")\n",
    "\n",
    "    # Encoder\n",
    "    enc_outputs = create_encoder(input_mix, num_layers, num_initial_filters, filter_size, input_filter_size, padding)\n",
    "\n",
    "    # Decoder\n",
    "    current_layer = create_decoder(enc_outputs, num_layers, num_initial_filters, filter_size, merge_filter_size, padding, upsampling)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = get_output_layer(current_layer, output_type, source_names, num_channels, output_filter_size, padding, activation, training)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs=input_mix, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_RATE = 22050\n",
    "SNIPPET_LENGTH = 16384  # Length of random snippets\n",
    "AUGMENTATION = True    # Toggle data augmentation\n",
    "# Time Jittering\n",
    "def time_jitter(audio, max_offset=500):\n",
    "    offset = np.random.randint(max_offset)\n",
    "    augmented_audio = np.pad(audio, (offset, 0), \"constant\")\n",
    "    return augmented_audio[:len(audio)]\n",
    "\n",
    "# Noise Injection\n",
    "def add_noise(audio, noise_level=0.005):\n",
    "    noise = np.random.randn(len(audio))\n",
    "    augmented_audio = audio + noise_level * noise\n",
    "    return np.clip(augmented_audio, -1, 1)\n",
    "\n",
    "# Reverb (simple decay)\n",
    "def add_reverb(audio, decay=0.5):\n",
    "    impulse_response = np.zeros(len(audio))\n",
    "    impulse_response[::4000] = decay\n",
    "    augmented_audio = np.convolve(audio, impulse_response, mode='same')\n",
    "    return np.clip(augmented_audio, -1, 1)\n",
    "\n",
    "# Random Cropping\n",
    "def random_cropping(audio, segment_length=SNIPPET_LENGTH):\n",
    "    start = np.random.randint(0, len(audio) - segment_length)\n",
    "    return audio[start: start + segment_length]\n",
    "\n",
    "# Frequency Masking (in the spectrogram domain)\n",
    "def freq_masking(spec, F=30, num_masks=1):\n",
    "    num_channels, num_frames = spec.shape\n",
    "    for _ in range(num_masks):\n",
    "        f = np.random.uniform(low=0.0, high=F)\n",
    "        f = int(f)\n",
    "        f0 = np.random.uniform(low=0.0, high=num_channels - f)\n",
    "        f0 = int(f0)\n",
    "        spec[f0:f0 + f, :] = 0\n",
    "    return spec\n",
    "\n",
    "# Time Masking (in the spectrogram domain)\n",
    "def time_masking(spec, T=40, num_masks=1):\n",
    "    num_channels, num_frames = spec.shape\n",
    "    for _ in range(num_masks):\n",
    "        t = np.random.uniform(low=0.0, high=T)\n",
    "        t = int(t)\n",
    "        t0 = np.random.uniform(low=0.0, high=num_frames - t)\n",
    "        t0 = int(t0)\n",
    "        spec[:, t0:t0 + t] = 0\n",
    "    return spec\n",
    "\n",
    "def random_amplify(audio):\n",
    "    factor = random.uniform(0.7, 1.3)  # Random amplification factor\n",
    "    return audio * factor\n",
    "\n",
    "def load_and_process_data(directory, min_mix=2, max_mix=5, augmentations={}):\n",
    "    X = []\n",
    "    y = []\n",
    "    vocal_dir = os.path.join(directory, '08Vox')\n",
    "    other_dirs = [os.path.join(directory, folder) for folder in os.listdir(directory) if folder != '08Vox' and not folder.startswith('.')]\n",
    "\n",
    "    for vocal_file in os.listdir(vocal_dir):\n",
    "        if not vocal_file.lower().endswith(('.wav', '.mp3', '.flac')):\n",
    "            continue\n",
    "        \n",
    "        vocal_path = os.path.join(vocal_dir, vocal_file)\n",
    "        vocal_signal, _ = librosa.load(vocal_path, sr=SAMPLE_RATE)\n",
    "\n",
    "        # Skip if the length is shorter than the snippet length\n",
    "        if len(vocal_signal) < SNIPPET_LENGTH:\n",
    "            continue\n",
    "\n",
    "        # Apply augmentations\n",
    "        if \"time_jitter\" in augmentations and augmentations[\"time_jitter\"]:\n",
    "            vocal_signal = time_jitter(vocal_signal)\n",
    "        if \"noise_injection\" in augmentations and augmentations[\"noise_injection\"]:\n",
    "            vocal_signal = add_noise(vocal_signal)\n",
    "        if \"reverb\" in augmentations and augmentations[\"reverb\"]:\n",
    "            vocal_signal = add_reverb(vocal_signal)\n",
    "        if \"random_cropping\" in augmentations and augmentations[\"random_cropping\"]:\n",
    "            vocal_signal = random_cropping(vocal_signal)\n",
    "\n",
    "        # Normalize the vocal signal\n",
    "        vocal_signal = normalize_audio(vocal_signal)\n",
    "\n",
    "        # Randomly select a number of mixes\n",
    "        num_mixes = random.randint(min_mix, max_mix)\n",
    "\n",
    "        mixed_signal = vocal_signal.copy()  # Create a copy of the vocal signal to be mixed\n",
    "\n",
    "        # Randomly select other samples to mix with the vocal\n",
    "        for _ in range(num_mixes):\n",
    "            other_dir = random.choice(other_dirs)\n",
    "            other_file = random.choice([f for f in os.listdir(other_dir) if f.lower().endswith(('.wav', '.mp3', '.flac'))])\n",
    "            other_path = os.path.join(other_dir, other_file)\n",
    "            other_signal, _ = librosa.load(other_path, sr=SAMPLE_RATE)\n",
    "\n",
    "            # Skip if the length is shorter than the snippet length\n",
    "            if len(other_signal) < SNIPPET_LENGTH:\n",
    "                continue\n",
    "\n",
    "            other_signal = normalize_audio(other_signal)\n",
    "            other_signal = pad_or_crop(other_signal, target_length=len(mixed_signal))\n",
    "            mixed_signal += other_signal\n",
    "\n",
    "        # Apply Frequency and Time Masking on the spectrogram\n",
    "        S = librosa.stft(vocal_signal)\n",
    "        if \"freq_masking\" in augmentations and augmentations[\"freq_masking\"]:\n",
    "            S = freq_masking(S)\n",
    "        if \"time_masking\" in augmentations and augmentations[\"time_masking\"]:\n",
    "            S = time_masking(S)\n",
    "\n",
    "        # Convert back to time domain\n",
    "        vocal_signal = librosa.istft(S)\n",
    "\n",
    "        # Divide into segments of 16384 samples\n",
    "        for i in range(0, len(vocal_signal), SNIPPET_LENGTH):\n",
    "            vocal_segment = pad_or_crop(vocal_signal[i:i + SNIPPET_LENGTH], SNIPPET_LENGTH)\n",
    "            mixed_segment = pad_or_crop(mixed_signal[i:i + SNIPPET_LENGTH], SNIPPET_LENGTH)\n",
    "\n",
    "            X.append(mixed_segment)\n",
    "            y.append(vocal_segment)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def pad_or_crop(audio, target_length):\n",
    "    length = len(audio)\n",
    "    if length < target_length:\n",
    "        padding = target_length - length\n",
    "        audio = np.pad(audio, (0, padding), 'constant')\n",
    "    elif length > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    return audio\n",
    "\n",
    "def normalize_audio(audio):\n",
    "    return 2 * (audio - np.min(audio)) / (np.max(audio) - np.min(audio)) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the data for training and testing\n",
    "augmentation_config = {\n",
    "    \"time_jitter\": True,\n",
    "    \"noise_injection\": True,\n",
    "    \"reverb\": False,\n",
    "    \"random_cropping\": True,\n",
    "    \"freq_masking\": True,\n",
    "    \"time_masking\": True\n",
    "}\n",
    "X_train, y_train = load_and_process_data(trainDir, augmentations=augmentation_config)\n",
    "X_test, y_test = load_and_process_data(testDir, augmentations=augmentation_config)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(X, y, batch_size=32, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(X))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Load and process the data for training and testing\n",
    "X_train, y_train = load_and_process_data(trainDir)\n",
    "X_test, y_test = load_and_process_data(testDir)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "batch_size = 64\n",
    "train_dataset = create_dataset(X_train, y_train, batch_size=batch_size)\n",
    "val_dataset = create_dataset(X_val, y_val, batch_size=batch_size)\n",
    "test_dataset = create_dataset(X_test, y_test, batch_size=batch_size, shuffle=False)  # No shuffling for test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries to load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Specifying the path to the trained model\n",
    "model_path = \"/Users/rei/Documents/Machine_Learning/MODELS/Unet/Shaking Through Sound Seperation/Shaking_Through_Model_01.keras\"\n",
    "\n",
    "Unet_Model = load_model(model_path, compile=False)\n",
    "\n",
    "# Compile the model again with your desired optimizer\n",
    "Unet_Model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model = build_model()  # Function that builds your model architecture\n",
    "model.load_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "callbacks = [reduce_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreinert-wasserman\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rei/Documents/Machine_Learning/Tutorials/Hugging Face/Tutorials/Audio/wandb/run-20230811_015925-todlvn92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reinert-wasserman/Shaking_Through_Unet_model/runs/todlvn92' target=\"_blank\">clear-water-5</a></strong> to <a href='https://wandb.ai/reinert-wasserman/Shaking_Through_Unet_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reinert-wasserman/Shaking_Through_Unet_model' target=\"_blank\">https://wandb.ai/reinert-wasserman/Shaking_Through_Unet_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reinert-wasserman/Shaking_Through_Unet_model/runs/todlvn92' target=\"_blank\">https://wandb.ai/reinert-wasserman/Shaking_Through_Unet_model/runs/todlvn92</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.init(project='Shaking_Through_Unet_model')\n",
    "callbacks.append(WandbCallback(log_weights=True))\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint('model.h5', save_best_only=True, monitor='val_loss')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "callbacks.extend([checkpoint, early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(train_dataset, validation_data\u001b[39m=\u001b[39mval_dataset, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, callbacks\u001b[39m=\u001b[39mcallbacks)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, callbacks=callbacks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

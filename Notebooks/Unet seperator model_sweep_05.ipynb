{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, LeakyReLU, UpSampling1D, Concatenate, Subtract\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Cropping1D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.nn import sigmoid\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_folder = \"/Users/rei/Documents/Machine_Learning/MODELS/Unet/Unet_Sound_Seperation/Unet-Sound-Seperation/Models/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(tensor, target_shape, match_feature_dim=True):\n",
    "    shape = tf.shape(tensor)\n",
    "    diff = shape - target_shape\n",
    "    assert diff[1] >= 0 # Only positive difference allowed\n",
    "    if diff[1] == 0:\n",
    "        return tensor\n",
    "    crop_start = diff // 2\n",
    "    crop_end = diff - crop_start\n",
    "    return tensor[:, crop_start[1]:-crop_end[1], :]\n",
    "\n",
    "def AudioClip(x, training):\n",
    "    if training:\n",
    "        return x\n",
    "    else:\n",
    "        return tf.maximum(tf.minimum(x, 1.0), -1.0)\n",
    "\n",
    "def difference_output(input_mix, featuremap, source_names, num_channels, filter_width, padding, activation, training):\n",
    "    outputs = dict()\n",
    "    sum_source = 0\n",
    "    for name in source_names[:-1]:\n",
    "        out = tf.keras.layers.Conv1D(num_channels, filter_width, activation=activation, padding=padding)(featuremap)\n",
    "        outputs[name] = out\n",
    "        sum_source += out\n",
    "\n",
    "    last_source = crop(input_mix, sum_source.shape) - sum_source\n",
    "    last_source = AudioClip(last_source, training)\n",
    "    outputs[source_names[-1]] = last_source\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learned_interpolation_layer(input, padding, level):\n",
    "    features = input.shape[2]\n",
    "    weights = tf.Variable(tf.initializers.GlorotUniform()(shape=[features]), dtype=tf.float32, name=\"interp_\" + str(level))\n",
    "    weights_scaled = tf.nn.sigmoid(weights)\n",
    "    counter_weights = 1.0 - weights_scaled\n",
    "\n",
    "    conv_weights = tf.linalg.diag(weights_scaled)\n",
    "    conv_weights = tf.expand_dims(conv_weights, axis=0)\n",
    "    intermediate_vals = tf.linalg.matmul(input, conv_weights)\n",
    "    \n",
    "    counter_conv_weights = tf.linalg.diag(counter_weights)\n",
    "    counter_conv_weights = tf.expand_dims(counter_conv_weights, axis=0)\n",
    "    counter_intermediate_vals = tf.linalg.matmul(input, counter_conv_weights)\n",
    "\n",
    "    output = tf.concat([intermediate_vals, counter_intermediate_vals], axis=1)\n",
    "    \n",
    "    if padding == \"valid\":\n",
    "        output = output[:, :-1, :]\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(input, num_layers, num_initial_filters, filter_size, input_filter_size, padding, dropout_rate=0.3):\n",
    "    enc_outputs = []\n",
    "    current_layer = input\n",
    "    current_layer = tf.keras.layers.Conv1D(num_initial_filters, input_filter_size, strides=1, activation=LeakyReLU(), padding=padding)(current_layer)\n",
    "    current_layer = tf.keras.layers.Dropout(dropout_rate)(current_layer)  # Adding dropout here\n",
    "    enc_outputs.append(current_layer)\n",
    "\n",
    "    for i in range(num_layers - 1):\n",
    "        current_layer = tf.keras.layers.Conv1D(num_initial_filters + (num_initial_filters * i), filter_size, strides=1, activation=LeakyReLU(), padding=padding)(current_layer)\n",
    "        current_layer = tf.keras.layers.Dropout(dropout_rate)(current_layer)  # Adding dropout here\n",
    "        current_layer = current_layer[:, ::2, :]  # Decimate by factor of 2\n",
    "        enc_outputs.append(current_layer)\n",
    "\n",
    "    return enc_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(enc_outputs, num_layers, num_initial_filters, filter_size, merge_filter_size, padding, upsampling, dropout_rate=0.3):\n",
    "    current_layer = enc_outputs[-1]\n",
    "\n",
    "    for i in range(num_layers - 1, 0, -1):\n",
    "        if upsampling == 'linear':\n",
    "            current_layer = tf.keras.layers.UpSampling1D(size=2)(current_layer)\n",
    "        elif upsampling == 'learned':\n",
    "            current_layer = learned_interpolation_layer(current_layer, padding=padding, level=i)\n",
    "\n",
    "        current_layer = tf.concat([current_layer, enc_outputs[i - 1]], axis=2)\n",
    "        current_layer = tf.keras.layers.Conv1D(num_initial_filters * (num_layers - i), merge_filter_size, strides=1, activation=LeakyReLU(), padding=padding)(current_layer)\n",
    "        current_layer = tf.keras.layers.Dropout(dropout_rate)(current_layer)  # Adding dropout here\n",
    "\n",
    "    return current_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layer(current_layer, output_type, source_names, num_channels, output_filter_size, padding, activation, training):\n",
    "    if output_type == \"direct\":\n",
    "        return independent_outputs(current_layer, source_names, num_channels, output_filter_size, padding, activation)\n",
    "    elif output_type == \"difference\":\n",
    "        cropped_input = crop(input, current_layer.get_shape().as_list(), match_feature_dim=False)\n",
    "        return difference_output(cropped_input, current_layer, source_names, num_channels, output_filter_size, padding, activation, training)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown output type\")\n",
    "\n",
    "def independent_outputs(featuremap, source_names, num_channels, filter_width, padding, activation):\n",
    "    outputs = dict()\n",
    "    for name in source_names:\n",
    "        outputs[name] = tf.keras.layers.Conv1D(num_channels, filter_width, activation=activation, padding=padding)(featuremap)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_layers, num_initial_filters, filter_size, input_filter_size):\n",
    "    # Input\n",
    "    input_mix = Input(shape=(num_frames, num_channels), name=\"input\")\n",
    "\n",
    "    # Encoder\n",
    "    enc_outputs = create_encoder(input_mix, num_layers, num_initial_filters, filter_size, input_filter_size, padding)\n",
    "\n",
    "    # Decoder\n",
    "    current_layer = create_decoder(enc_outputs, num_layers, num_initial_filters, filter_size, merge_filter_size, padding, upsampling)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = get_output_layer(current_layer, output_type, source_names, num_channels, output_filter_size, padding, activation, training)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs=input_mix, outputs=outputs)\n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "SNIPPET_LENGTH = 16384\n",
    "tfRecord_Datasets = '/Users/rei/Documents/Machine_Learning/Data/Audio/Shaking_Through/Generated_datasets/tf_Record'\n",
    "\n",
    "\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        'mixed_signal': tf.io.FixedLenFeature([SNIPPET_LENGTH], tf.float32),\n",
    "        'vocal_signal': tf.io.FixedLenFeature([SNIPPET_LENGTH], tf.float32)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    return example['mixed_signal'], example['vocal_signal']\n",
    "\n",
    "def load_dataset(filename):\n",
    "    raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "    return raw_dataset.map(parse_tfrecord_fn)\n",
    "\n",
    "# Load your training, validation and test data\n",
    "train_dataset = load_dataset(os.path.join(tfRecord_Datasets, 'train.tfrecord'))\n",
    "val_dataset = load_dataset(os.path.join(tfRecord_Datasets, 'val.tfrecord'))\n",
    "test_dataset = load_dataset(os.path.join(tfRecord_Datasets, 'test.tfrecord'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreinert-wasserman\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rei/Documents/Machine_Learning/MODELS/Unet/Unet_Sound_Seperation/Unet-Sound-Seperation/Notebooks/wandb/run-20230811_215844-d1whe1at</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reinert-wasserman/Unet-Sound-Seperation-Notebooks/runs/d1whe1at' target=\"_blank\">light-sky-1</a></strong> to <a href='https://wandb.ai/reinert-wasserman/Unet-Sound-Seperation-Notebooks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reinert-wasserman/Unet-Sound-Seperation-Notebooks' target=\"_blank\">https://wandb.ai/reinert-wasserman/Unet-Sound-Seperation-Notebooks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reinert-wasserman/Unet-Sound-Seperation-Notebooks/runs/d1whe1at' target=\"_blank\">https://wandb.ai/reinert-wasserman/Unet-Sound-Seperation-Notebooks/runs/d1whe1at</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[1;32m     30\u001b[0m wandb\u001b[39m.\u001b[39minit(config\u001b[39m=\u001b[39msweep_config)\n\u001b[0;32m---> 32\u001b[0m wandb\u001b[39m.\u001b[39mlog({\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: train_loss, \u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m: val_loss, \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m: accuracy})\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m EarlyStopping\n\u001b[1;32m     35\u001b[0m earlystop_callback \u001b[39m=\u001b[39m EarlyStopping(\n\u001b[1;32m     36\u001b[0m   monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'name': 'unet-separator-sweep',\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'num_layers': {\n",
    "            'values': [6, 12, 15]\n",
    "        },\n",
    "        'num_initial_filters': {\n",
    "            'values': [12, 24, 48]\n",
    "        },\n",
    "        'filter_size': {\n",
    "            'values': [7, 15, 30]\n",
    "        },\n",
    "        'BATCH_SIZE': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.002, 0.001, 0.0005]\n",
    "        },\n",
    "        'lr_scheduler': {\n",
    "            'values': ['ReduceLROnPlateau', 'ExponentialDecay']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "import wandb\n",
    "wandb.init(config=sweep_config)\n",
    "\n",
    "wandb.log({'loss': train_loss, 'val_loss': val_loss, 'accuracy': accuracy})\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop_callback = EarlyStopping(\n",
    "  monitor='val_loss', min_delta=0.0001, patience=3)\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(0.01, 20)\n",
    "lr_callback = LearningRateScheduler(exponential_decay_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 16384\n",
    "num_channels = 1\n",
    "num_layers = wandb.config.num_layers\n",
    "num_initial_filters = wandb.config.num_initial_filters\n",
    "filter_size = wandb.config.filter_size\n",
    "merge_filter_size = 5\n",
    "input_filter_size = 15\n",
    "output_filter_size = 1\n",
    "padding = 'same'  \n",
    "upsampling = 'linear'  # or 'learned'\n",
    "output_type = 'direct'  # or 'difference'\n",
    "source_names = [\"accompaniment\", \"vocals\"]\n",
    "activation = 'tanh'\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Shaking_Through_Unet_model\")\n",
    "wandb.agent(sweep_id, function=train_function)  # Replace train_function with the name of your training function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function():\n",
    "    # Initialize wandb\n",
    "    run = wandb.init()\n",
    "    config = run.config\n",
    "\n",
    "    # Define your model using the hyperparameters from wandb's config\n",
    "model = build_model(config.num_layers, config.num_initial_filters, config.filter_size, config.input_filter_size)\n",
    "\n",
    "    # Define your optimizer and compile model\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=config.learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse', 'mae', 'accuracy'])  # Added Mean Absolute Error (mae) and accuracy as metrics\n",
    "\n",
    "    # Define your callbacks including EarlyStopping and LearningRateScheduler\n",
    "    callbacks_list = [wandb.keras.WandbCallback()]  # WandB's callback to log metrics\n",
    "    earlystop_callback = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3)\n",
    "    callbacks_list.append(earlystop_callback)\n",
    "\n",
    "    if config.lr_scheduler == 'ReduceLROnPlateau':\n",
    "        lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n",
    "    elif config.lr_scheduler == 'ExponentialDecay':\n",
    "        exponential_decay_fn = exponential_decay(0.01, 20)\n",
    "        lr_callback = LearningRateScheduler(exponential_decay_fn)\n",
    "    callbacks_list.append(lr_callback)\n",
    "\n",
    "    # Train your model\n",
    "    model.fit(train_dataset.batch(config.BATCH_SIZE),\n",
    "              validation_data=val_dataset.batch(config.BATCH_SIZE), \n",
    "              epochs=10, \n",
    "              batch_size=config.BATCH_SIZE, \n",
    "              callbacks=callbacks_list)\n",
    "\n",
    "    # Close the run\n",
    "    run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

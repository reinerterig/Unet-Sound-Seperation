{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, LeakyReLU, UpSampling1D, Concatenate, Subtract\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Cropping1D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.nn import sigmoid\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/rei/Documents/Machine_Learning/MODELS/Unet/Shaking Through Sound Seperation/Shaking_Through_Model_01.keras\"\n",
    "trainDir = '/Users/rei/Documents/Machine_Learning/Data/Audio/Shaking_Through/Dataset/Train'\n",
    "testDir = '/Users/rei/Documents/Machine_Learning/Data/Audio/Shaking_Through/Dataset/Test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(tensor, target_shape, match_feature_dim=True):\n",
    "    shape = tf.shape(tensor)\n",
    "    diff = shape - target_shape\n",
    "    assert diff[1] >= 0 # Only positive difference allowed\n",
    "    if diff[1] == 0:\n",
    "        return tensor\n",
    "    crop_start = diff // 2\n",
    "    crop_end = diff - crop_start\n",
    "    return tensor[:, crop_start[1]:-crop_end[1], :]\n",
    "\n",
    "def AudioClip(x, training):\n",
    "    if training:\n",
    "        return x\n",
    "    else:\n",
    "        return tf.maximum(tf.minimum(x, 1.0), -1.0)\n",
    "\n",
    "def difference_output(input_mix, featuremap, source_names, num_channels, filter_width, padding, activation, training):\n",
    "    outputs = dict()\n",
    "    sum_source = 0\n",
    "    for name in source_names[:-1]:\n",
    "        out = tf.keras.layers.Conv1D(num_channels, filter_width, activation=activation, padding=padding)(featuremap)\n",
    "        outputs[name] = out\n",
    "        sum_source += out\n",
    "\n",
    "    last_source = crop(input_mix, sum_source.shape) - sum_source\n",
    "    last_source = AudioClip(last_source, training)\n",
    "    outputs[source_names[-1]] = last_source\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learned_interpolation_layer(input, padding, level):\n",
    "    features = input.shape[2]\n",
    "    weights = tf.Variable(tf.initializers.GlorotUniform()(shape=[features]), dtype=tf.float32, name=\"interp_\" + str(level))\n",
    "    weights_scaled = tf.nn.sigmoid(weights)\n",
    "    counter_weights = 1.0 - weights_scaled\n",
    "\n",
    "    conv_weights = tf.linalg.diag(weights_scaled)\n",
    "    conv_weights = tf.expand_dims(conv_weights, axis=0)\n",
    "    intermediate_vals = tf.linalg.matmul(input, conv_weights)\n",
    "    \n",
    "    counter_conv_weights = tf.linalg.diag(counter_weights)\n",
    "    counter_conv_weights = tf.expand_dims(counter_conv_weights, axis=0)\n",
    "    counter_intermediate_vals = tf.linalg.matmul(input, counter_conv_weights)\n",
    "\n",
    "    output = tf.concat([intermediate_vals, counter_intermediate_vals], axis=1)\n",
    "    \n",
    "    if padding == \"valid\":\n",
    "        output = output[:, :-1, :]\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(input, num_layers, num_initial_filters, filter_size, input_filter_size, padding):\n",
    "    enc_outputs = []\n",
    "    current_layer = input\n",
    "    current_layer = tf.keras.layers.Conv1D(num_initial_filters, input_filter_size, strides=1, activation=LeakyReLU(), padding=padding)(current_layer)\n",
    "    enc_outputs.append(current_layer)\n",
    "\n",
    "    for i in range(num_layers - 1):\n",
    "        current_layer = tf.keras.layers.Conv1D(num_initial_filters + (num_initial_filters * i), filter_size, strides=1, activation=LeakyReLU(), padding=padding)(current_layer)\n",
    "        current_layer = current_layer[:, ::2, :]  # Decimate by factor of 2\n",
    "        enc_outputs.append(current_layer)\n",
    "\n",
    "    return enc_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(enc_outputs, num_layers, num_initial_filters, filter_size, merge_filter_size, padding, upsampling):\n",
    "    current_layer = enc_outputs[-1]\n",
    "\n",
    "    for i in range(num_layers - 1, 0, -1):\n",
    "        if upsampling == 'linear':\n",
    "            current_layer = tf.keras.layers.UpSampling1D(size=2)(current_layer)\n",
    "        elif upsampling == 'learned':\n",
    "            current_layer = learned_interpolation_layer(current_layer, padding=padding, level=i)\n",
    "\n",
    "        current_layer = tf.concat([current_layer, enc_outputs[i - 1]], axis=2)\n",
    "        current_layer = tf.keras.layers.Conv1D(num_initial_filters * (num_layers - i), merge_filter_size, strides=1, activation=LeakyReLU(), padding=padding)(current_layer)\n",
    "\n",
    "    return current_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layer(current_layer, output_type, source_names, num_channels, output_filter_size, padding, activation, training):\n",
    "    if output_type == \"direct\":\n",
    "        return independent_outputs(current_layer, source_names, num_channels, output_filter_size, padding, activation)\n",
    "    elif output_type == \"difference\":\n",
    "        cropped_input = crop(input, current_layer.get_shape().as_list(), match_feature_dim=False)\n",
    "        return difference_output(cropped_input, current_layer, source_names, num_channels, output_filter_size, padding, activation, training)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown output type\")\n",
    "\n",
    "def independent_outputs(featuremap, source_names, num_channels, filter_width, padding, activation):\n",
    "    outputs = dict()\n",
    "    for name in source_names:\n",
    "        outputs[name] = tf.keras.layers.Conv1D(num_channels, filter_width, activation=activation, padding=padding)(featuremap)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 16384\n",
    "num_channels = 1\n",
    "num_layers = 12\n",
    "num_initial_filters = 24\n",
    "filter_size = 15\n",
    "merge_filter_size = 5\n",
    "input_filter_size = 15\n",
    "output_filter_size = 1\n",
    "padding = 'same'  \n",
    "upsampling = 'linear'  # or 'learned'\n",
    "output_type = 'direct'  # or 'difference'\n",
    "source_names = [\"accompaniment\", \"vocals\"]\n",
    "activation = 'tanh'\n",
    "training = True\n",
    "\n",
    "def build_model():\n",
    "    # Input\n",
    "    input_mix = Input(shape=(num_frames, num_channels), name=\"input\")\n",
    "\n",
    "    # Encoder\n",
    "    enc_outputs = create_encoder(input_mix, num_layers, num_initial_filters, filter_size, input_filter_size, padding)\n",
    "\n",
    "    # Decoder\n",
    "    current_layer = create_decoder(enc_outputs, num_layers, num_initial_filters, filter_size, merge_filter_size, padding, upsampling)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = get_output_layer(current_layer, output_type, source_names, num_channels, output_filter_size, padding, activation, training)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs=input_mix, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_RATE = 22050\n",
    "SNIPPET_LENGTH = 16384  # Length of random snippets\n",
    "AUGMENTATION = True    # Toggle data augmentation\n",
    "\n",
    "def random_amplify(audio):\n",
    "    factor = random.uniform(0.7, 1.3)  # Random amplification factor\n",
    "    return audio * factor\n",
    "\n",
    "def load_and_process_data(directory, min_mix=2, max_mix=5):\n",
    "    X = []\n",
    "    y = []\n",
    "    vocal_dir = os.path.join(directory, '08Vox')\n",
    "    other_dirs = [os.path.join(directory, folder) for folder in os.listdir(directory) if folder != '08Vox' and not folder.startswith('.')]\n",
    "    \n",
    "    for vocal_file in os.listdir(vocal_dir):\n",
    "        if not vocal_file.lower().endswith(('.wav', '.mp3', '.flac')):\n",
    "            continue\n",
    "        \n",
    "        vocal_path = os.path.join(vocal_dir, vocal_file)\n",
    "        vocal_signal, _ = librosa.load(vocal_path, sr=SAMPLE_RATE)\n",
    "\n",
    "        # Skip if the length is shorter than the snippet length\n",
    "        if len(vocal_signal) < SNIPPET_LENGTH:\n",
    "            continue\n",
    "\n",
    "        # Normalize the vocal signal\n",
    "        vocal_signal = normalize_audio(vocal_signal)\n",
    "\n",
    "        # Randomly select a number of mixes\n",
    "        num_mixes = random.randint(min_mix, max_mix)\n",
    "\n",
    "        mixed_signal = vocal_signal.copy()  # Create a copy of the vocal signal to be mixed\n",
    "\n",
    "        # Randomly select other samples to mix with the vocal\n",
    "        for _ in range(num_mixes):\n",
    "            other_dir = random.choice(other_dirs)\n",
    "            other_file = random.choice([f for f in os.listdir(other_dir) if f.lower().endswith(('.wav', '.mp3', '.flac'))])\n",
    "            other_path = os.path.join(other_dir, other_file)\n",
    "            other_signal, _ = librosa.load(other_path, sr=SAMPLE_RATE)\n",
    "\n",
    "            # Skip if the length is shorter than the snippet length\n",
    "            if len(other_signal) < SNIPPET_LENGTH:\n",
    "                continue\n",
    "\n",
    "            other_signal = normalize_audio(other_signal)\n",
    "            other_signal = pad_or_crop(other_signal, target_length=len(mixed_signal))\n",
    "            mixed_signal += other_signal\n",
    "\n",
    "        # Divide into segments of 16384 samples\n",
    "        for i in range(0, len(vocal_signal), SNIPPET_LENGTH):\n",
    "            vocal_segment = pad_or_crop(vocal_signal[i:i + SNIPPET_LENGTH], SNIPPET_LENGTH)\n",
    "            mixed_segment = pad_or_crop(mixed_signal[i:i + SNIPPET_LENGTH], SNIPPET_LENGTH)\n",
    "\n",
    "            X.append(mixed_segment)\n",
    "            y.append(vocal_segment)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def pad_or_crop(audio, target_length):\n",
    "    length = len(audio)\n",
    "    if length < target_length:\n",
    "        padding = target_length - length\n",
    "        audio = np.pad(audio, (0, padding), 'constant')\n",
    "    elif length > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    return audio\n",
    "\n",
    "def normalize_audio(audio):\n",
    "    return 2 * (audio - np.min(audio)) / (np.max(audio) - np.min(audio)) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the data for training and testing\n",
    "X_train, y_train = load_and_process_data(trainDir)\n",
    "X_test, y_test = load_and_process_data(testDir)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries to load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Specifying the path to the trained model\n",
    "model_path = \"/Users/rei/Documents/Machine_Learning/MODELS/Unet/Shaking Through Sound Seperation/Shaking_Through_Model_01.keras\"\n",
    "\n",
    "Unet_Model = load_model(model_path, compile=False)\n",
    "\n",
    "# Compile the model again with your desired optimizer\n",
    "Unet_Model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model = build_model()  # Function that builds your model architecture\n",
    "model.load_weights(model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

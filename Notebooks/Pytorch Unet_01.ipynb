{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_frames = 16384\n",
    "num_channels = 1\n",
    "num_layers = 12\n",
    "num_initial_filters = 24\n",
    "filter_size = 15\n",
    "merge_filter_size = 5\n",
    "input_filter_size = 15\n",
    "output_filter_size = 1\n",
    "padding = 'same'  \n",
    "upsampling = 'linear'  # or 'learned'\n",
    "output_type = 'direct'  # or 'difference'\n",
    "source_names = [\"accompaniment\", \"vocals\"]\n",
    "activation = 'tanh'\n",
    "training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crop(tensor, target_shape):\n",
    "    \"\"\"\n",
    "    Crop the tensor to match the target shape. \n",
    "    It is assumed that the tensor shape and target shape only differ in the time dimension.\n",
    "    \"\"\"\n",
    "    shape = tensor.shape\n",
    "    target_length = target_shape[2]\n",
    "    tensor_length = shape[2]\n",
    "    \n",
    "    start = (tensor_length - target_length) // 2\n",
    "    end = start + target_length\n",
    "\n",
    "    return tensor[:, :, start:end]\n",
    "\n",
    "\n",
    "def AudioClip(x, training):\n",
    "    if training:\n",
    "        return x\n",
    "    else:\n",
    "        return torch.clamp(x, -1.0, 1.0)\n",
    "\n",
    "def difference_output(input_mix, featuremap, source_names, num_channels, filter_width, padding, activation, training):\n",
    "    outputs = {}\n",
    "    sum_source = 0\n",
    "    \n",
    "    # Convert padding type from 'same' or 'valid' to PyTorch's 'same' or 'valid' equivalent\n",
    "    padding_val = (filter_width - 1) // 2 if padding == 'same' else 0\n",
    "    \n",
    "    for name in source_names[:-1]:\n",
    "        conv_layer = nn.Conv1d(featuremap.shape[1], num_channels, filter_width, padding=padding_val)\n",
    "        out = conv_layer(featuremap)\n",
    "        \n",
    "        if activation == 'tanh':\n",
    "            out = torch.tanh(out)\n",
    "        # Add other activation checks here if needed\n",
    "        \n",
    "        outputs[name] = out\n",
    "        sum_source += out\n",
    "\n",
    "    # Use the crop function to ensure the shapes match\n",
    "    last_source = crop(input_mix, sum_source.shape) - sum_source\n",
    "    last_source = AudioClip(last_source, training)\n",
    "    outputs[source_names[-1]] = last_source\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LearnedInterpolationLayer(nn.Module):\n",
    "    def __init__(self, features, padding, level):\n",
    "        super(LearnedInterpolationLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(features)))\n",
    "        self.padding = padding\n",
    "        self.level = level\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights_scaled = torch.sigmoid(self.weights)\n",
    "        counter_weights = 1.0 - weights_scaled\n",
    "\n",
    "        # Create diagonal matrices for weights and counter-weights\n",
    "        conv_weights = torch.diag(weights_scaled).unsqueeze(0)\n",
    "        counter_conv_weights = torch.diag(counter_weights).unsqueeze(0)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        intermediate_vals = torch.bmm(x, conv_weights)\n",
    "        counter_intermediate_vals = torch.bmm(x, counter_conv_weights)\n",
    "\n",
    "        # Concatenate along the channel dimension\n",
    "        output = torch.cat([intermediate_vals, counter_intermediate_vals], dim=1)\n",
    "\n",
    "        if self.padding == \"valid\":\n",
    "            output = output[:, :, :-1]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 8192])\n",
      "torch.Size([1, 48, 4096])\n",
      "torch.Size([1, 72, 2048])\n",
      "torch.Size([1, 96, 1024])\n",
      "torch.Size([1, 120, 512])\n",
      "torch.Size([1, 144, 256])\n",
      "torch.Size([1, 168, 128])\n",
      "torch.Size([1, 192, 64])\n",
      "torch.Size([1, 216, 32])\n",
      "torch.Size([1, 240, 16])\n",
      "torch.Size([1, 264, 8])\n",
      "torch.Size([1, 288, 8])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_initial_filters, num_layers, filter_size, input_filter_size, padding, dropout_rate=0.3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input convolution layer\n",
    "        self.layers.append(nn.Conv1d(1, num_initial_filters, input_filter_size, stride=1, padding=self.get_padding(padding, input_filter_size)))\n",
    "        self.layers.append(nn.LeakyReLU(0.01))\n",
    "        self.layers.append(nn.Dropout(dropout_rate))\n",
    "      \n",
    "        # Loop for adding convolution layers\n",
    "        for i in range(num_layers - 1):\n",
    "            in_channels = num_initial_filters * (i + 1)\n",
    "            out_channels = in_channels + num_initial_filters\n",
    "            self.layers.append(nn.Conv1d(in_channels, out_channels, filter_size, stride=1, padding=self.get_padding(padding, filter_size)))\n",
    "            self.layers.append(nn.LeakyReLU(0.01))\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_outputs = []\n",
    "        for i in range(0, 3 * self.num_layers, 3):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.layers[i + 1](x)\n",
    "            x = self.layers[i + 2](x)\n",
    "            if i < 3 * (self.num_layers - 1):\n",
    "                x = x[:, :, ::2]  # Decimate by factor of 2\n",
    "            enc_outputs.append(x)\n",
    "        return enc_outputs\n",
    "    \n",
    "    def get_padding(self, padding_type, kernel_size):\n",
    "        return (kernel_size - 1) // 2 if padding_type == 'same' else 0\n",
    "\n",
    "# Instantiate and test\n",
    "encoder = Encoder(num_initial_filters, num_layers, filter_size, input_filter_size, padding)\n",
    "dummy_input = torch.randn(1, 1, 16384)  # Batch size of 1, 1 channel, 16384 frames\n",
    "enc_out = encoder(dummy_input)\n",
    "for out in enc_out:\n",
    "    print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_frames, num_channels, num_layers, num_initial_filters, filter_size, merge_filter_size, \n",
    "                 input_filter_size, output_filter_size, padding, upsampling, output_type, source_names, activation, training):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.upsampling = upsampling\n",
    "        self.output_type = output_type\n",
    "        self.source_names = source_names\n",
    "        self.training = training\n",
    "        \n",
    "        # Define the encoder layers\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.encoders.append(nn.Conv1d(num_channels, num_initial_filters, input_filter_size, stride=1, padding=self.get_padding(padding, input_filter_size)))\n",
    "        self.encoders.append(nn.Dropout(0.3))\n",
    "        for i in range(num_layers - 1):\n",
    "            self.encoders.append(nn.Conv1d(num_initial_filters + (num_initial_filters * i), \n",
    "                                           num_initial_filters + (num_initial_filters * (i + 1)), \n",
    "                                           filter_size, stride=1, padding=self.get_padding(padding, filter_size)))\n",
    "            self.encoders.append(nn.Dropout(0.3))\n",
    "        \n",
    "        # Define the decoder layers\n",
    "        self.decoders = nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.decoders.append(nn.Conv1d(num_initial_filters * (num_layers - i + 1), num_initial_filters * (num_layers - i), merge_filter_size, stride=1, padding=self.get_padding(padding, merge_filter_size)))\n",
    "            self.decoders.append(nn.Dropout(0.3))\n",
    "        \n",
    "        # Define the final output layer\n",
    "        self.outputs = nn.ModuleList()\n",
    "        for _ in source_names:\n",
    "            self.outputs.append(nn.Conv1d(num_initial_filters, num_channels, output_filter_size, stride=1, padding=self.get_padding(padding, output_filter_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc_outputs = []\n",
    "        for i in range(0, 2 * self.num_layers, 2):\n",
    "            x = self.encoders[i](x)\n",
    "            x = F.leaky_relu(x, 0.01)\n",
    "            x = self.encoders[i + 1](x)\n",
    "            enc_outputs.append(x)\n",
    "            if i < 2 * (self.num_layers - 1):\n",
    "                x = x[:, :, ::2]  # Decimate by factor of 2\n",
    "        \n",
    "        # Decoder\n",
    "        for i in range(0, 2 * (self.num_layers - 1), 2):\n",
    "            if self.upsampling == 'linear':\n",
    "                x = F.interpolate(x, scale_factor=2)\n",
    "            elif self.upsampling == 'learned':\n",
    "                # Implement learned interpolation if needed\n",
    "                pass\n",
    "            x = torch.cat([x, enc_outputs[-(i//2 + 1)]], dim=1)\n",
    "            x = self.decoders[i](x)\n",
    "            x = F.leaky_relu(x, 0.01)\n",
    "            x = self.decoders[i + 1](x)\n",
    "        \n",
    "        # Output Layer\n",
    "        outputs = {}\n",
    "        for i, name in enumerate(self.source_names):\n",
    "            outputs[name] = torch.tanh(self.outputs[i](x))\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def get_padding(self, padding_type, kernel_size):\n",
    "        return (kernel_size - 1) // 2 if padding_type == 'same' else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_frames = 16384\n",
    "num_channels = 1\n",
    "num_layers = 12\n",
    "num_initial_filters = 24\n",
    "filter_size = 15\n",
    "\n",
    "merge_filter_size = 5\n",
    "input_filter_size = 15\n",
    "output_filter_size = 1\n",
    "padding = 'same'  \n",
    "upsampling = 'linear'  # or 'learned'\n",
    "output_type = 'direct'  # or 'difference'\n",
    "source_names = [\"accompaniment\", \"vocals\"]\n",
    "activation = 'tanh'\n",
    "training = True\n",
    "\n",
    "model = UNet(num_frames, num_channels, num_layers, num_initial_filters, filter_size, merge_filter_size, \n",
    "             input_filter_size, output_filter_size, padding, upsampling, output_type, source_names, activation, training)\n",
    "\n",
    "# If needed, define the loss and optimizer for training\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 16384])\n",
      "After encoder Conv[0]: torch.Size([1, 24, 16384])\n",
      "After decimation[0]: torch.Size([1, 24, 8192])\n",
      "After encoder Conv[1]: torch.Size([1, 48, 8192])\n",
      "After decimation[1]: torch.Size([1, 48, 4096])\n",
      "After encoder Conv[2]: torch.Size([1, 72, 4096])\n",
      "After decimation[2]: torch.Size([1, 72, 2048])\n",
      "After encoder Conv[3]: torch.Size([1, 96, 2048])\n",
      "After decimation[3]: torch.Size([1, 96, 1024])\n",
      "After encoder Conv[4]: torch.Size([1, 120, 1024])\n",
      "After decimation[4]: torch.Size([1, 120, 512])\n",
      "After encoder Conv[5]: torch.Size([1, 144, 512])\n",
      "After decimation[5]: torch.Size([1, 144, 256])\n",
      "After encoder Conv[6]: torch.Size([1, 168, 256])\n",
      "After decimation[6]: torch.Size([1, 168, 128])\n",
      "After encoder Conv[7]: torch.Size([1, 192, 128])\n",
      "After decimation[7]: torch.Size([1, 192, 64])\n",
      "After encoder Conv[8]: torch.Size([1, 216, 64])\n",
      "After decimation[8]: torch.Size([1, 216, 32])\n",
      "After encoder Conv[9]: torch.Size([1, 240, 32])\n",
      "After decimation[9]: torch.Size([1, 240, 16])\n",
      "After encoder Conv[10]: torch.Size([1, 264, 16])\n",
      "After decimation[10]: torch.Size([1, 264, 8])\n",
      "After encoder Conv[11]: torch.Size([1, 288, 8])\n",
      "After upsampling[0]: torch.Size([1, 288, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 16 but got size 4 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m debug_model \u001b[39m=\u001b[39m DebugUNet(num_frames, num_channels, num_layers, num_initial_filters, filter_size, merge_filter_size, \n\u001b[1;32m     44\u001b[0m                         input_filter_size, output_filter_size, padding, upsampling, output_type, source_names, activation, training)\n\u001b[1;32m     46\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, num_channels, num_frames)  \u001b[39m# Batch size of 1\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m output \u001b[39m=\u001b[39m debug_model(dummy_input)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchGPU/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[26], line 27\u001b[0m, in \u001b[0;36mDebugUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     26\u001b[0m cropped_enc_output \u001b[39m=\u001b[39m crop(enc_outputs[\u001b[39m-\u001b[39m(i\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)], x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 27\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([x, cropped_enc_output], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAfter concatenation[\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m]:\u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     29\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoders[i](x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 16 but got size 4 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "class DebugUNet(UNet):\n",
    "    def forward(self, x):\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        \n",
    "        # Encoder\n",
    "        enc_outputs = []\n",
    "        for i in range(0, 2 * self.num_layers, 2):\n",
    "            x = self.encoders[i](x)\n",
    "            print(f\"After encoder Conv[{i//2}]:\", x.shape)\n",
    "            x = F.leaky_relu(x, 0.01)\n",
    "            x = self.encoders[i + 1](x)\n",
    "            enc_outputs.append(x)\n",
    "            if i < 2 * (self.num_layers - 1):\n",
    "                x = x[:, :, ::2]  # Decimate by factor of 2\n",
    "                print(f\"After decimation[{i//2}]:\", x.shape)\n",
    "        \n",
    "        # Decoder\n",
    "        for i in range(0, 2 * (self.num_layers - 1), 2):\n",
    "            if self.upsampling == 'linear':\n",
    "                x = F.interpolate(x, scale_factor=2)\n",
    "                print(f\"After upsampling[{i//2}]:\", x.shape)\n",
    "            elif self.upsampling == 'learned':\n",
    "                # Implement learned interpolation if needed\n",
    "                pass\n",
    "            \n",
    "            cropped_enc_output = crop(enc_outputs[-(i//2 + 1)], x.shape)\n",
    "            x = torch.cat([x, cropped_enc_output], dim=1)\n",
    "            print(f\"After concatenation[{i//2}]:\", x.shape)\n",
    "            x = self.decoders[i](x)\n",
    "            print(f\"After decoder Conv[{i//2}]:\", x.shape)\n",
    "            x = F.leaky_relu(x, 0.01)\n",
    "            x = self.decoders[i + 1](x)\n",
    "        \n",
    "        # Output Layer\n",
    "        outputs = {}\n",
    "        for i, name in enumerate(self.source_names):\n",
    "            outputs[name] = torch.tanh(self.outputs[i](x))\n",
    "            print(f\"Output shape of {name}:\", outputs[name].shape)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "\n",
    "debug_model = DebugUNet(num_frames, num_channels, num_layers, num_initial_filters, filter_size, merge_filter_size, \n",
    "                        input_filter_size, output_filter_size, padding, upsampling, output_type, source_names, activation, training)\n",
    "\n",
    "dummy_input = torch.randn(1, num_channels, num_frames)  # Batch size of 1\n",
    "output = debug_model(dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 16 but got size 8 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Assuming your model is designed for a specific input size\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input_size \u001b[39m=\u001b[39m (num_channels, num_frames)\n\u001b[0;32m----> 5\u001b[0m summary(model, input_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchGPU/lib/python3.11/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[1;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchGPU/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 87\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsampling \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlearned\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     85\u001b[0m     \u001b[39m# Implement learned interpolation if needed\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([x, enc_outputs[\u001b[39m-\u001b[39;49m(i\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)]], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     88\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoders[i](x)\n\u001b[1;32m     89\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mleaky_relu(x, \u001b[39m0.01\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 16 but got size 8 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Assuming your model is designed for a specific input size\n",
    "input_size = (num_channels, num_frames)\n",
    "summary(model, input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install torchsummary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, LeakyReLU, UpSampling1D, Concatenate, Subtract\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Cropping1D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.nn import sigmoid\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = '/Users/rei/Documents/Machine_Learning/Data/Audio/Shaking_Through/Dataset/Train'\n",
    "testDir = '/Users/rei/Documents/Machine_Learning/Data/Audio/Shaking_Through/Dataset/Test'\n",
    "tfRecord_Datasets = '/Users/rei/Documents/Machine_Learning/MODELS/Unet/Unet_Sound_Seperation/Unet-Sound-Seperation/tf_Record'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_RATE = 22050\n",
    "SNIPPET_LENGTH = 16384 *2  # Length of random snippets\n",
    "AUGMENTATION = True    # Toggle data augmentation\n",
    "# Time Jittering\n",
    "def time_jitter(audio, max_offset=500):\n",
    "    offset = np.random.randint(max_offset)\n",
    "    augmented_audio = np.pad(audio, (offset, 0), \"constant\")\n",
    "    return augmented_audio[:len(audio)]\n",
    "\n",
    "# Noise Injection\n",
    "def add_noise(audio, noise_level=0.005):\n",
    "    noise = np.random.randn(len(audio))\n",
    "    augmented_audio = audio + noise_level * noise\n",
    "    return np.clip(augmented_audio, -1, 1)\n",
    "\n",
    "# Reverb (simple decay)\n",
    "def add_reverb(audio, decay=0.5):\n",
    "    impulse_response = np.zeros(len(audio))\n",
    "    impulse_response[::4000] = decay\n",
    "    augmented_audio = np.convolve(audio, impulse_response, mode='same')\n",
    "    return np.clip(augmented_audio, -1, 1)\n",
    "\n",
    "# Random Cropping\n",
    "def random_cropping(audio, segment_length=SNIPPET_LENGTH):\n",
    "    start = np.random.randint(0, len(audio) - segment_length)\n",
    "    return audio[start: start + segment_length]\n",
    "\n",
    "# Frequency Masking (in the spectrogram domain)\n",
    "def freq_masking(spec, F=30, num_masks=1):\n",
    "    num_channels, num_frames = spec.shape\n",
    "    for _ in range(num_masks):\n",
    "        f = np.random.uniform(low=0.0, high=F)\n",
    "        f = int(f)\n",
    "        f0 = np.random.uniform(low=0.0, high=num_channels - f)\n",
    "        f0 = int(f0)\n",
    "        spec[f0:f0 + f, :] = 0\n",
    "    return spec\n",
    "\n",
    "# Time Masking (in the spectrogram domain)\n",
    "def time_masking(spec, T=40, num_masks=1):\n",
    "    num_channels, num_frames = spec.shape\n",
    "    for _ in range(num_masks):\n",
    "        t = np.random.uniform(low=0.0, high=T)\n",
    "        t = int(t)\n",
    "        t0 = np.random.uniform(low=0.0, high=num_frames - t)\n",
    "        t0 = int(t0)\n",
    "        spec[:, t0:t0 + t] = 0\n",
    "    return spec\n",
    "\n",
    "def random_amplify(audio):\n",
    "    factor = random.uniform(0.7, 1.3)  # Random amplification factor\n",
    "    return audio * factor\n",
    "\n",
    "def load_and_process_data(directory, min_mix=2, max_mix=5, augmentations={}):\n",
    "    X = []\n",
    "    y = []\n",
    "    vocal_dir = os.path.join(directory, '08Vox')\n",
    "    other_dirs = [os.path.join(directory, folder) for folder in os.listdir(directory) if folder != '08Vox' and not folder.startswith('.')]\n",
    "\n",
    "    for vocal_file in os.listdir(vocal_dir):\n",
    "        if not vocal_file.lower().endswith(('.wav', '.mp3', '.flac')):\n",
    "            continue\n",
    "        \n",
    "        vocal_path = os.path.join(vocal_dir, vocal_file)\n",
    "        vocal_signal, _ = librosa.load(vocal_path, sr=SAMPLE_RATE)\n",
    "\n",
    "        # Skip if the length is shorter than the snippet length\n",
    "        if len(vocal_signal) < SNIPPET_LENGTH:\n",
    "            continue\n",
    "\n",
    "        # Apply augmentations\n",
    "        if \"time_jitter\" in augmentations and augmentations[\"time_jitter\"]:\n",
    "            vocal_signal = time_jitter(vocal_signal)\n",
    "        if \"noise_injection\" in augmentations and augmentations[\"noise_injection\"]:\n",
    "            vocal_signal = add_noise(vocal_signal)\n",
    "        if \"reverb\" in augmentations and augmentations[\"reverb\"]:\n",
    "            vocal_signal = add_reverb(vocal_signal)\n",
    "        if \"random_cropping\" in augmentations and augmentations[\"random_cropping\"]:\n",
    "            vocal_signal = random_cropping(vocal_signal)\n",
    "\n",
    "        # Normalize the vocal signal\n",
    "        vocal_signal = normalize_audio(vocal_signal)\n",
    "\n",
    "        # Randomly select a number of mixes\n",
    "        num_mixes = random.randint(min_mix, max_mix)\n",
    "\n",
    "        mixed_signal = vocal_signal.copy()  # Create a copy of the vocal signal to be mixed\n",
    "\n",
    "        # Randomly select other samples to mix with the vocal\n",
    "        for _ in range(num_mixes):\n",
    "            other_dir = random.choice(other_dirs)\n",
    "            other_file = random.choice([f for f in os.listdir(other_dir) if f.lower().endswith(('.wav', '.mp3', '.flac'))])\n",
    "            other_path = os.path.join(other_dir, other_file)\n",
    "            other_signal, _ = librosa.load(other_path, sr=SAMPLE_RATE)\n",
    "\n",
    "            # Skip if the length is shorter than the snippet length\n",
    "            if len(other_signal) < SNIPPET_LENGTH:\n",
    "                continue\n",
    "\n",
    "            other_signal = normalize_audio(other_signal)\n",
    "            other_signal = pad_or_crop(other_signal, target_length=len(mixed_signal))\n",
    "            mixed_signal += other_signal\n",
    "\n",
    "        # Apply Frequency and Time Masking on the spectrogram\n",
    "        S = librosa.stft(vocal_signal)\n",
    "        if \"freq_masking\" in augmentations and augmentations[\"freq_masking\"]:\n",
    "            S = freq_masking(S)\n",
    "        if \"time_masking\" in augmentations and augmentations[\"time_masking\"]:\n",
    "            S = time_masking(S)\n",
    "\n",
    "        # Convert back to time domain\n",
    "        vocal_signal = librosa.istft(S)\n",
    "\n",
    "        # Divide into segments of 16384 samples\n",
    "        for i in range(0, len(vocal_signal), SNIPPET_LENGTH):\n",
    "            vocal_segment = pad_or_crop(vocal_signal[i:i + SNIPPET_LENGTH], SNIPPET_LENGTH)\n",
    "            mixed_segment = pad_or_crop(mixed_signal[i:i + SNIPPET_LENGTH], SNIPPET_LENGTH)\n",
    "\n",
    "            X.append(mixed_segment)\n",
    "            y.append(vocal_segment)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def pad_or_crop(audio, target_length):\n",
    "    length = len(audio)\n",
    "    if length < target_length:\n",
    "        padding = target_length - length\n",
    "        audio = np.pad(audio, (0, padding), 'constant')\n",
    "    elif length > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    return audio\n",
    "\n",
    "def normalize_audio(audio):\n",
    "    return 2 * (audio - np.min(audio)) / (np.max(audio) - np.min(audio)) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the data for training and testing\n",
    "augmentation_config = {\n",
    "    \"time_jitter\": True,\n",
    "    \"noise_injection\": True,\n",
    "    \"reverb\": False,\n",
    "    \"random_cropping\": True,\n",
    "    \"freq_masking\": True,\n",
    "    \"time_masking\": True\n",
    "}\n",
    "X_train, y_train = load_and_process_data(trainDir, augmentations=augmentation_config)\n",
    "X_test, y_test = load_and_process_data(testDir, augmentations=augmentation_config)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def serialize_example(mixed_signal, vocal_signal):\n",
    "    feature = {\n",
    "        'mixed_signal': _float_feature(mixed_signal),\n",
    "        'vocal_signal': _float_feature(vocal_signal)\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def write_tfrecord(filename, X, y):\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for i in range(len(X)):\n",
    "            example = serialize_example(X[i].flatten(), y[i].flatten())\n",
    "            writer.write(example)\n",
    "\n",
    "# Save your training, validation and test data\n",
    "write_tfrecord(os.path.join(tfRecord_Datasets,'train_2.tfrecord'), X_train, y_train)\n",
    "write_tfrecord(os.path.join(tfRecord_Datasets,'val_2.tfrecord'), X_val, y_val)\n",
    "write_tfrecord(os.path.join(tfRecord_Datasets,'test_2.tfrecord'), X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
